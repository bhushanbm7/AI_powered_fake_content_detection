{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1b5f4d3-3851-4499-ba5f-35e6c7882f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image_path', 'label'],\n",
      "        num_rows: 1632\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['image_path', 'label'],\n",
      "        num_rows: 409\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "def load_faceforensics(data_dir):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    \n",
    "    # 'real' images are given label 0, 'fake' images are given label 1\n",
    "    for label, category in enumerate([\"real\", \"fake\"]):\n",
    "        category_path = os.path.join(data_dir, category)\n",
    "        for img_name in os.listdir(category_path):\n",
    "            # Optionally, you can filter for image files here\n",
    "            if img_name.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "                image_paths.append(os.path.join(category_path, img_name))\n",
    "                labels.append(label)\n",
    "    \n",
    "    df = pd.DataFrame({\"image_path\": image_paths, \"label\": labels})\n",
    "    return df\n",
    "\n",
    "# Update data_dir with the path to your FaceForensics++ folder\n",
    "data_dir = \"C:/Users/hruti/OneDrive/Desktop/tp/FaceForensics++\"\n",
    "df = load_faceforensics(data_dir)\n",
    "\n",
    "# Convert the DataFrame into a Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3b545b5-da66-42c4-9f7d-a0fd0e8acb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "\n",
    "model_name = \"prithivMLmods/Deepfake-Detection-Exp-02-21\"\n",
    "processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "model = AutoModelForImageClassification.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e05aa90d-ca91-4d8e-bef6-8c356ab4a602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9e6ac02713f44f2828e0ad79bed5a9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1632 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a31e5d888d1f42129e233ca283f36d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/409 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Define your transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "def preprocess_function(example):\n",
    "    # Open the image file and convert it to RGB\n",
    "    image = Image.open(example[\"image_path\"]).convert(\"RGB\")\n",
    "    # Apply transformations to get pixel values\n",
    "    example[\"pixel_values\"] = transform(image)\n",
    "    return example\n",
    "\n",
    "# Apply the transformation to your dataset and remove the original image path column\n",
    "dataset = dataset.map(preprocess_function, remove_columns=[\"image_path\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1488fe1-574b-4d0f-ad8d-7bf775778638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: tensorflow<2.20,>=2.19 in c:\\users\\hruti\\anaconda3\\envs\\eco\\lib\\site-packages (from tf-keras) (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\hruti\\anaconda3\\envs\\eco\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\hruti\\anaconda3\\envs\\eco\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\hruti\\anaconda3\\envs\\eco\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\hruti\\anaconda3\\envs\\eco\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\hruti\\anaconda3\\envs\\eco\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\hruti\\anaconda3\\envs\\eco\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\hruti\\anaconda3\\envs\\eco\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\hruti\\anaconda3\\envs\\eco\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\hruti\\anaconda3\\envs\\eco\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (5.29.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\hruti\\anaconda3\\envs\\eco\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hruti\\anaconda3\\envs\\eco\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\hruti\\anaconda3\\envs\\eco\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\hruti\\anaconda3\\envs\\eco\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\hruti\\anaconda3\\envs\\eco\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\hruti\\anaconda3\\envs\\eco\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\hruti\\anaconda3\\envs\\eco\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\hruti\\anaconda3\\envs\\eco\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\hruti\\anaconda3\\envs\\eco\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.9.0)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\users\\hruti\\anaconda3\\envs\\eco\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.1.3)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\hruti\\anaconda3\\envs\\eco\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\hruti\\anaconda3\\envs\\eco\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\hruti\\anaconda3\\envs\\eco\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\hruti\\anaconda3\\envs\\eco\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19->tf-keras) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\hruti\\anaconda3\\envs\\eco\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\users\\hruti\\anaconda3\\envs\\eco\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\hruti\\anaconda3\\envs\\eco\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hruti\\anaconda3\\envs\\eco\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hruti\\anaconda3\\envs\\eco\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hruti\\anaconda3\\envs\\eco\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hruti\\anaconda3\\envs\\eco\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\hruti\\anaconda3\\envs\\eco\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\hruti\\anaconda3\\envs\\eco\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\hruti\\anaconda3\\envs\\eco\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\hruti\\anaconda3\\envs\\eco\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\hruti\\anaconda3\\envs\\eco\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\hruti\\anaconda3\\envs\\eco\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\hruti\\anaconda3\\envs\\eco\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.1.2)\n",
      "Downloading tf_keras-2.19.0-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ------------------------ --------------- 1.0/1.7 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 3.7 MB/s eta 0:00:00\n",
      "Installing collected packages: tf-keras\n",
      "Successfully installed tf-keras-2.19.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9cb5ba93-9d7c-4f4e-9a47-85cefd529af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp310-cp310-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\hruti\\anaconda3\\envs\\eco\\lib\\site-packages (from scikit-learn) (2.1.3)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.2-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.6.1-cp310-cp310-win_amd64.whl (11.1 MB)\n",
      "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/11.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/11.1 MB 1.7 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 1.0/11.1 MB 1.6 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 1.6/11.1 MB 1.8 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 1.8/11.1 MB 1.8 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 2.4/11.1 MB 1.9 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 2.9/11.1 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 3.4/11.1 MB 2.1 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 3.9/11.1 MB 2.2 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 4.7/11.1 MB 2.3 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 5.5/11.1 MB 2.4 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 6.0/11.1 MB 2.5 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 7.1/11.1 MB 2.6 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 7.9/11.1 MB 2.7 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 8.7/11.1 MB 2.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.4/11.1 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.5/11.1 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.1/11.1 MB 3.0 MB/s eta 0:00:00\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading scipy-1.15.2-cp310-cp310-win_amd64.whl (41.2 MB)\n",
      "   ---------------------------------------- 0.0/41.2 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.8/41.2 MB 3.3 MB/s eta 0:00:13\n",
      "   - -------------------------------------- 1.8/41.2 MB 4.2 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 2.6/41.2 MB 4.2 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 3.1/41.2 MB 3.6 MB/s eta 0:00:11\n",
      "   --- ------------------------------------ 3.7/41.2 MB 3.8 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 4.7/41.2 MB 3.8 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 5.2/41.2 MB 3.6 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 5.8/41.2 MB 3.5 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 6.6/41.2 MB 3.4 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 7.1/41.2 MB 3.5 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 7.6/41.2 MB 3.4 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 7.9/41.2 MB 3.4 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 8.7/41.2 MB 3.2 MB/s eta 0:00:11\n",
      "   -------- ------------------------------- 9.2/41.2 MB 3.2 MB/s eta 0:00:11\n",
      "   --------- ------------------------------ 10.0/41.2 MB 3.2 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 10.5/41.2 MB 3.2 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 11.3/41.2 MB 3.2 MB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 11.8/41.2 MB 3.1 MB/s eta 0:00:10\n",
      "   ------------ --------------------------- 12.6/41.2 MB 3.2 MB/s eta 0:00:10\n",
      "   ------------ --------------------------- 13.1/41.2 MB 3.2 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 13.9/41.2 MB 3.1 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 14.4/41.2 MB 3.1 MB/s eta 0:00:09\n",
      "   -------------- ------------------------- 15.2/41.2 MB 3.1 MB/s eta 0:00:09\n",
      "   --------------- ------------------------ 15.7/41.2 MB 3.1 MB/s eta 0:00:09\n",
      "   --------------- ------------------------ 16.3/41.2 MB 3.1 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 17.0/41.2 MB 3.1 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 17.8/41.2 MB 3.2 MB/s eta 0:00:08\n",
      "   ------------------ --------------------- 18.6/41.2 MB 3.2 MB/s eta 0:00:08\n",
      "   ------------------ --------------------- 19.4/41.2 MB 3.2 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 19.9/41.2 MB 3.2 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 20.7/41.2 MB 3.2 MB/s eta 0:00:07\n",
      "   --------------------- ------------------ 21.8/41.2 MB 3.2 MB/s eta 0:00:07\n",
      "   --------------------- ------------------ 22.3/41.2 MB 3.2 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 22.8/41.2 MB 3.2 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 23.6/41.2 MB 3.2 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 24.4/41.2 MB 3.2 MB/s eta 0:00:06\n",
      "   ------------------------ --------------- 24.9/41.2 MB 3.2 MB/s eta 0:00:06\n",
      "   ------------------------ --------------- 25.7/41.2 MB 3.2 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 26.2/41.2 MB 3.2 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 27.0/41.2 MB 3.2 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 27.5/41.2 MB 3.2 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 27.8/41.2 MB 3.2 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 28.6/41.2 MB 3.2 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 29.1/41.2 MB 3.2 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 29.6/41.2 MB 3.1 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 30.1/41.2 MB 3.1 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 30.7/41.2 MB 3.1 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 31.5/41.2 MB 3.1 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 32.0/41.2 MB 3.1 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 32.5/41.2 MB 3.1 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 33.0/41.2 MB 3.1 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 33.6/41.2 MB 3.1 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 34.3/41.2 MB 3.1 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 34.6/41.2 MB 3.1 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 35.4/41.2 MB 3.1 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 36.2/41.2 MB 3.1 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 37.0/41.2 MB 3.1 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 37.5/41.2 MB 3.1 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 38.0/41.2 MB 3.1 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 38.5/41.2 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 39.1/41.2 MB 3.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 39.6/41.2 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  40.4/41.2 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  40.9/41.2 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 41.2/41.2 MB 3.1 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.1 scipy-1.15.2 threadpoolctl-3.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6628b39e-961c-4c95-8651-2938c4043041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='612' max='612' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [612/612 52:34, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.186800</td>\n",
       "      <td>0.625201</td>\n",
       "      <td>0.750611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.032600</td>\n",
       "      <td>0.793238</td>\n",
       "      <td>0.770171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>0.828029</td>\n",
       "      <td>0.787286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=612, training_loss=0.1765014311554385, metrics={'train_runtime': 3160.3677, 'train_samples_per_second': 1.549, 'train_steps_per_second': 0.194, 'total_flos': 3.794007811530424e+17, 'train_loss': 0.1765014311554385, 'epoch': 3.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Define a compute_metrics function to calculate accuracy\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\"accuracy\": acc}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./deepfake-finetuned\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    eval_strategy=\"epoch\",            # Updated strategy (use eval_strategy instead of evaluation_strategy if needed)\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,        # Load the best model when finished\n",
    "    metric_for_best_model=\"accuracy\",   # Metric used for early stopping\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,     # Pass the compute_metrics function\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=1)]\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d155d91-6cb1-43d0-94a2-dcfb6f5a5d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fine-tuned-deepfake-detector\\\\preprocessor_config.json']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save locally\n",
    "model.save_pretrained(\"fine-tuned-deepfake-detector\")\n",
    "processor.save_pretrained(\"fine-tuned-deepfake-detector\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c61d33e-2d66-42a7-9f69-a224c3efd9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\hruti\\anaconda3\\envs\\eco\\lib\\site-packages (0.29.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\hruti\\anaconda3\\envs\\eco\\lib\\site-packages (from huggingface_hub) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\hruti\\anaconda3\\envs\\eco\\lib\\site-packages (from huggingface_hub) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\hruti\\anaconda3\\envs\\eco\\lib\\site-packages (from huggingface_hub) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hruti\\anaconda3\\envs\\eco\\lib\\site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\hruti\\anaconda3\\envs\\eco\\lib\\site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\hruti\\anaconda3\\envs\\eco\\lib\\site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hruti\\anaconda3\\envs\\eco\\lib\\site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\hruti\\anaconda3\\envs\\eco\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hruti\\anaconda3\\envs\\eco\\lib\\site-packages (from requests->huggingface_hub) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hruti\\anaconda3\\envs\\eco\\lib\\site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hruti\\anaconda3\\envs\\eco\\lib\\site-packages (from requests->huggingface_hub) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hruti\\anaconda3\\envs\\eco\\lib\\site-packages (from requests->huggingface_hub) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "970024d0-909d-4169-ba25-77e67f2fc5e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a405df3f4314786a17553ed44e46b4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02448b81d4ee41a89c9a6c158a0cfa32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b33fa1ccaee4dd68fcced6572ff5ee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/343M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89d72e11986544e1b3a7edc922a5bceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/HrutikAdsare/deepfake-detector-faceforensics/commit/0bda52a3902284b6e6867d55acbfc1592886b862', commit_message='Upload processor', commit_description='', oid='0bda52a3902284b6e6867d55acbfc1592886b862', pr_url=None, repo_url=RepoUrl('https://huggingface.co/HrutikAdsare/deepfake-detector-faceforensics', endpoint='https://huggingface.co', repo_type='model', repo_id='HrutikAdsare/deepfake-detector-faceforensics'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()  # This will prompt you for your Hugging Face token\n",
    "\n",
    "model.push_to_hub(\"HrutikAdsare/deepfake-detector-faceforensics\")\n",
    "processor.push_to_hub(\"HrutikAdsare/deepfake-detector-faceforensics\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3d5829c-bd8c-4b4f-aab7-93d9ada6ec2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bff092b3d5b64ba3bd88f3a4791df4e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/374 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fc0c7f1b5a844d382e8d43b8e4ceea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/770 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1e975ed531f4c1e8f2ccd037eb2d9f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/343M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and processor loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "\n",
    "model_name = \"HrutikAdsare/deepfake-detector-faceforensics\"\n",
    "processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "model = AutoModelForImageClassification.from_pretrained(model_name)\n",
    "\n",
    "# Now you can perform inference as usual\n",
    "print(\"Model and processor loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a4cb49-726f-4afc-8b78-dd38df99f829",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
